{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple flask webapp to try out our NER based heuristic function that returns news similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import redirect\n",
    "\n",
    "#iinstall spacy in your conda conda install -c conda-forge spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil.parser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "Weights = {'CARDINAL': 1,\n",
    " 'DATE': 1,\n",
    " 'EVENT': 1,\n",
    " 'FAC':  2,\n",
    " 'GPE': 2,\n",
    " 'LANGUAGE': 1,\n",
    " 'LAW': 2,\n",
    " 'LOC': 2,\n",
    " 'MONEY': 4,\n",
    " 'NORP': 1,\n",
    " 'ORDINAL': 1,\n",
    " 'ORG': 16,\n",
    " 'PERCENT': 16,\n",
    " 'PERSON': 16,\n",
    " 'PRODUCT': 4,\n",
    " 'QUANTITY': 2,\n",
    " 'TIME': 1,\n",
    " 'WORK_OF_ART': 4\n",
    "          }\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def intersection(x, y):\n",
    "    r = set.intersection(x, y).difference(['Join Livemint', 'Telegram', 'Mint'])\n",
    "    return r\n",
    "\n",
    "\n",
    "def weighted_score(inter):\n",
    "    l = list(inter)\n",
    "    score = 0\n",
    "    for i in l:\n",
    "        score += Weights[i.split(':')[0]]\n",
    "    return score\n",
    "\n",
    "\n",
    "def vec_similarity(x, y):\n",
    "    if x is not None and y is not None:\n",
    "        return x.similarity(y)\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def tag(x):\n",
    "    s = set()\n",
    "    doc = nlp(x)\n",
    "    for ent in doc.ents:\n",
    "        s.add(ent.label_+\":\"+ent.text)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def heuristic(x, y): \n",
    "    intersect = intersection(tag(x), tag(y))\n",
    "    score = weighted_score(intersect)\n",
    "    vec_sim = vec_similarity(nlp(x), nlp(y))\n",
    "    verdict = 0\n",
    "    if (vec_sim>0.95) and len(intersect)>2 and (score>100):\n",
    "        verdict = 1\n",
    "    r = \"Score: \"+str(score)+ \"<br>Intersect: \"+str(intersect)+\"<br>Content Vector Similarity: \"+str(vec_sim)+ \"<br>Verdict: \"+ str(verdict)\n",
    "    return verdict, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "C:\\Users\\PARI\\Anaconda3\\lib\\threading.py:890: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  self._bootstrap_inner()\n",
      "127.0.0.1 - - [06/Dec/2020 18:09:56] \"GET /eval?x=San+Francisco+is+a+beautiful+city&y=San+Francisco+has+a+lot+of+beauty HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Dec/2020 18:10:03] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "def get_index():\n",
    "    return \"<form action=\\\"/eval\\\" method=\\\"get\\\" id=\\\"eval\\\"> \\\n",
    "      <textarea name=\\\"x\\\" rows=\\\"20\\\" cols=\\\"100\\\" form=\\\"eval\\\">Enter text X here...</textarea> \\\n",
    "      <textarea name=\\\"y\\\" rows=\\\"20\\\" cols=\\\"100\\\" form=\\\"eval\\\">Enter text Y here...</textarea> \\\n",
    "      <input type=\\\"submit\\\">\"\n",
    "    \n",
    "\n",
    "@app.route(\"/\")\n",
    "def init():\n",
    "    return get_index()\n",
    "\n",
    "\n",
    "@app.route(\"/eval\")\n",
    "def eval():\n",
    "    x =request.args.get('x')\n",
    "    y =request.args.get('y')\n",
    "    verdict, r = heuristic(x, y)\n",
    "    return r\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(row):\n",
    "    suffix = 'N'\n",
    "    if int(row.prediction) == 1:\n",
    "        suffix = 'P'\n",
    "    prefix = 'F'\n",
    "    if row.prediction == row['binary-label']:\n",
    "        prefix = 'T'\n",
    "    return prefix+suffix\n",
    "\n",
    "no_of = lambda x: len(result[result['confusion']==x])\n",
    "\n",
    "def print_quality(result):\n",
    "    result['prediction'] = result.apply(lambda row: on_row(row), axis=1)\n",
    "    result['confusion'] = result.apply(lambda row: confusion(row), axis = 1)\n",
    "    Accuracy = (no_of('TP') + no_of('TN'))/(no_of('TP') + no_of('TN') + no_of('FP') + no_of('FN'))\n",
    "    Precision = no_of('TP') / (no_of('TP') + no_of('FP'))\n",
    "    Recall = no_of('TP') / (no_of('TP') + no_of('FN'))\n",
    "    print('Accuracy = '+ str(Accuracy))\n",
    "    print('Precision = '+ str(Precision))\n",
    "    print('Recall = '+ str(Recall))\n",
    "\n",
    "def on_row(row):\n",
    "    x = row['content_x']\n",
    "    y = row['content_y']\n",
    "    verdict, r = heuristic(x, y)\n",
    "    return verdict\n",
    "\n",
    "test_data = 'D:\\\\newsapibackup\\\\benchmark-data_balance.csv'\n",
    "# test_data = 'D:\\\\newsapibackup\\\\benchmark-data_real.csv'\n",
    "result = pd.read_csv(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry reader. I need to suppress the warnings. They will be printed in a loop otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8\n",
      "Precision = 0.7380952380952381\n",
      "Recall = 0.6966292134831461\n"
     ]
    }
   ],
   "source": [
    "print_quality(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
