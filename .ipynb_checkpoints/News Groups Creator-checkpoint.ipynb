{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Similarity Problem - Detecting Paraphrases\n",
    "\n",
    "If you are developing a news aggregator you will face the issue of news similarity. A number of sources either report paraphrases of the same new story. At times more than 2 sources will send you literally the same text, picked up from a major agency such as Reuter.\n",
    "\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "A commercial NLP library such as spacy will have NER. This is useful for our purposes. If 2 news items are referring to the same peron, organization, event and has the same numbers then it is quite likely that they are paraphrases of each other.\n",
    "\n",
    "\n",
    "## This notebook demonstrates a heuristic technique using NER to group news items with the same meaning.\n",
    "\n",
    "# Further improvement\n",
    "With a large training data set one can fine tune this NER based algorithm by doing supervised learning over NER and below mentioned weighted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import redirect\n",
    "\n",
    "#iinstall spacy in your conda conda install -c conda-forge spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#iinstall spacy in your conda conda install -c conda-forge spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "Weights = {'CARDINAL': 1,\n",
    " 'DATE': 1,\n",
    " 'EVENT': 1,\n",
    " 'FAC':  2,\n",
    " 'GPE': 2,\n",
    " 'LANGUAGE': 1,\n",
    " 'LAW': 2,\n",
    " 'LOC': 2,\n",
    " 'MONEY': 4,\n",
    " 'NORP': 1,\n",
    " 'ORDINAL': 1,\n",
    " 'ORG': 16,\n",
    " 'PERCENT': 16,\n",
    " 'PERSON': 16,\n",
    " 'PRODUCT': 4,\n",
    " 'QUANTITY': 2,\n",
    " 'TIME': 1,\n",
    " 'WORK_OF_ART': 4\n",
    "          }\n",
    "\n",
    "common_junk_words = ['Join Livemint', 'Telegram', 'Mint']\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def intersection(x, y):\n",
    "    r = set.intersection(x, y).difference(common_junk_words)\n",
    "    return r\n",
    "\n",
    "\n",
    "def weighted_score(inter):\n",
    "    l = list(inter)\n",
    "    score = 0\n",
    "    for i in l:\n",
    "        score += Weights[i.split(':')[0]]\n",
    "    return score\n",
    "\n",
    "\n",
    "def vec_similarity(x, y):\n",
    "    if x is not None and y is not None:\n",
    "        return x.similarity(y)\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def tag(x):\n",
    "    s = set()\n",
    "    doc = nlp(x)\n",
    "    for ent in doc.ents:\n",
    "        s.add(ent.label_+\":\"+ent.text)\n",
    "    return s\n",
    "\n",
    "\n",
    "def heuristic(x, y): \n",
    "    intersect = intersection(tag(x), tag(y))\n",
    "    score = weighted_score(intersect)\n",
    "    vec_sim = vec_similarity(nlp(x), nlp(y))\n",
    "    verdict = 0\n",
    "    if (vec_sim>0.95) and len(intersect)>2 and (score>100):\n",
    "        verdict = 1\n",
    "    return verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "\n",
    "newsapi = NewsApiClient(api_key=os.environ.get(\"NEWS_API_KEY\"))\n",
    "\n",
    "page_size = 100\n",
    "    \n",
    "domains = 'reuters.com, bloomberg.com, indianexpress.com'\n",
    "\n",
    "def date_string(d):\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def get_all_articles(page):\n",
    "    to = datetime.now() - timedelta(days=1)\n",
    "    to_param = date_string(to)\n",
    "    from_param = date_string(to - timedelta(days=0))\n",
    "    all_articles = newsapi.get_everything(q='',\n",
    "                                          sources='',\n",
    "                                          domains= domains,\n",
    "                                          from_param=from_param,\n",
    "                                          to=to_param,\n",
    "                                          language='en',\n",
    "                                          sort_by='publishedAt',\n",
    "                                         page_size = page_size,\n",
    "                                         page = page)\n",
    "    return all_articles\n",
    "\n",
    "all_articles = get_all_articles(1)\n",
    "totalResults = int(all_articles['totalResults'])\n",
    "no_of_pages = int(totalResults/page_size) + 1\n",
    "no_of_pages\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "entities = []\n",
    "urls = []\n",
    "descriptions = []\n",
    "ids = []\n",
    "published_ats = []\n",
    "sources = []\n",
    "doc_vecs = []\n",
    "title_vecs = []\n",
    "\n",
    "id = 0\n",
    "for page in range(1,no_of_pages+1):\n",
    "    all_articles = get_all_articles(page)\n",
    "    for j in all_articles['articles']: \n",
    "        titles.append(j['title'])\n",
    "        contents.append(j['content'])\n",
    "        urls.append(j['url'])\n",
    "        ids.append(id)\n",
    "        descriptions.append(j['description'])\n",
    "        sources.append(j['source']['name'])\n",
    "        s = set()\n",
    "        doc = None\n",
    "        if(j['content'] != None):\n",
    "            doc = nlp(j['content'])\n",
    "            for ent in doc.ents:\n",
    "                s.add(ent.label_+\":\"+ent.text)\n",
    "        doc_vecs.append(doc) \n",
    "        entities.append(s)\n",
    "        published_ats.append(dateutil.parser.parse(j['publishedAt']))\n",
    "        tn = None\n",
    "        if(j['title'] != None):\n",
    "            tn = nlp(j['title'])\n",
    "        title_vecs.append(tn)\n",
    "        id += 1\n",
    "        \n",
    "data = {'title': titles, \n",
    "        'content' : contents,\n",
    "        'entities' : entities,\n",
    "        'url' : urls,\n",
    "        'description' : descriptions,\n",
    "        'id' : ids,\n",
    "        'published_at' : published_ats,\n",
    "        'source': sources,\n",
    "        'doc_vec': doc_vecs,\n",
    "        'title_vec': title_vecs \n",
    "       } \n",
    "df = pd.DataFrame(data) \n",
    "\n",
    "df['content'].replace('None', np.nan, inplace=True)\n",
    "df.dropna(subset=['content'], inplace=True)\n",
    "\n",
    "df['key'] = 1\n",
    "# to obtain the cross join we will merge  \n",
    "# on the key and drop it. \n",
    "result = pd.merge(df, df, on ='key').drop(\"key\", 1)\n",
    "result = result[(result.title_x != result.title_y)] \n",
    "\n",
    "def sorted_tuple(x, y):\n",
    "    if(x>y):\n",
    "        return y, x\n",
    "    return x, y\n",
    "    \n",
    "result['temp'] = result.apply(lambda row: sorted_tuple(row.id_x, row.id_y), axis=1)\n",
    "result = result.drop_duplicates(subset='temp', keep=\"first\")\n",
    "result =  result[(result.source_x != result.source_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['similar'] = result.apply(lambda row: heuristic(row['content_x'], row['content_y']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['similar'] = result[result['similar']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
